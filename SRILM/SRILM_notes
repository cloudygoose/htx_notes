main web-pages:
http://www.speech.sri.com/projects/srilm/manpages/
I follow the install steps on the web
==============================install===============================
In make, I find I can't find the installed iconv-lib, so I need to set the environment variable(found in the INSTALL manual, Kai pointed to us that it's a environment variable)
	NO_ICONV=1
And remember at this line in the Makefile, don't add any space!!!!!
	SRILM=/home/slhome/txh18/software/SRILM
When I install iconv, I installed it to /usr/local/lib，to run programs in SRILM/bin/i686-m64, I need to make links to /usr/lib
	sudo ln -s /usr/local/lib/libiconv.so.2 /usr/lib/libiconv.so.2
Add bin folder to $PATH
Add this line to the end of .bashrc
	export PATH=$PATH:/home/slhome/txh18/software/SRILM/bin/i686-m64
========================Issues's about the ARPA LM file===============
\data\
ngram 1=n1
ngram 2=n2
...
ngram N=nN

\1-grams:
p	w		[bow]
...

\2-grams:
p	w1 w2		[bow]
...

\N-grams:
p	w1 ... wN
...

\end\

Backoff weights are required only for those N-grams that form a prefix of longer N-grams in the model. The highest-order N-grams in particular will not need backoff weights (they would be useless).

Since log(0) (minus infinity) has no portable representation, such values are mapped to a large negative number. However, the designated dummy value (-99 in SRILM) is interpreted as log(0) when read back from file into memory.

unigram: log p(wi)     wi   log\alpha(wi)
bigram:  log p(w_i|w_{i-1})  w_{i-1}w_i   log\alpha(w_{i-1}w_i)
trigram: log p(w_i|w_{i-2}w_{i-1})  w_{i-2}w_{i-1}w_i
==============================Kai's scripts============================
1.The readme only has three parameters?
The README may be wrong, I guess the $3 should be ./textsources/testset.dat, and $4 should be ./weiboLM

It's put at
	scp -P 219 202.120.38.148:/slfs2/users/ky219/asr/wrdseg/lms/v0/README .
What I need is
	./README
	./train_tg_prune.sh
	./weibo.50k.wlist
	./textsources/testset.dat
	./textsources/weibo.dat
===ngram-count===
Most Important count options:
	-debug level
Set debugging output from estimated LM at level. Level 0 means no debugging. Debugging messages are written to stderr.
	-order n
Set the maximal order (length) of N-grams to count. This also determines the order of the estimated LM, if any. The default order is 3.
	-vocab file
Read a vocabulary from file. Subsequently, out-of-vocabulary words in both counts or text are replaced with the unknown-word token. If this option is not specified all words found are implicitly added to the vocabulary.
	-write-vocab file
Write the vocabulary built in the counting process to file.
	-gtnmin count
where n is 1, 2, 3, 4, 5, 6, 7, 8, or 9. Set the minimal count of N-grams of order n that will be included in the LM. All N-grams with frequency lower than that will effectively be discounted to 0. If n is omitted the parameter for N-grams of order > 9 is set. NOTE: This option affects not only the default Good-Turing discounting but the alternative discounting methods described below as well.
	-text textfile
Generate N-gram counts from text file. textfile should contain one sentence unit per line. Begin/end sentence tokens are added if not already present. Empty lines are ignored.
	-write file
Write total counts to file.
	-sort
Output counts in lexicographic order, as required for ngram-merge(1).
=EXAMPLE=
	ngram-count -text test_htx.dat -write1 htx_1_gram
I get:
<s>	3
Hello	1
my	1
name	1
is	1
Hetianxing	1
</s>	3
I	2
love	2
you	2
not	1
only	1

	ngram-count -text test_htx.dat -write2 htx_2_gram
I get:
<s> Hello	1
<s> I	2
Hello my	1
my name	1
name is	1
is Hetianxing	1
Hetianxing </s>	1
I love	2
love you	1
love not	1
you </s>	2
not only	1
only you	1

Notes About the LM options:
1.How does the backoff lm format represent the interpolate model??
The outputFile is in backoff format, even we use a interpolate model. Why? check SURVEYP19, because when count=0, the MLE component is just 0, so there's only (1-\namblda)*(p(^{i-1}_{i-n+2}), so it is consistent with the backoff format.

2.From Mail List - Why is C(<s>) always zero??
> Hello,
>
> is there a reason why the unigram count of the auto-prepended sentence 
> start tag <s> is always zero? As can be seen from the output below, 
> the log probabilities are calculated counting the sentence send tags 
> </s> but not the start tags. Or have I just missed something horribly 
> obvious?

You are confusing a token's frequency in the text with the probability 
in the model.
Because <s> only occurs as part of an ngram's history, but never as the 
token being predicted, its probability is 0.  If P(<s>) were > 0, then 
(via backoff) you would also have P(<s> | ...) > 0 and the sum of 
probabilities over all allowed words would be < 1.

If you want the unigram probability of a sentence boundary, use the </s> 
tag.

Andreas
Well, I guess <s> is somewhat special, we can't regard it as a normal word.

3.How is the parameter D in the KN discount set?
Kneser-Ney discounting. This is similar to absolute discounting in that the discounted probability is computed by subtracting a constant D from the N-gram count. The options -kndiscount and -ukndiscount differ as to how this constant is computed. 
The original Kneser-Ney discounting (-ukndiscount) uses one discounting constant for each N-gram order. These constants are estimated as

	D = n1 / (n1 + 2*n2)

where n1 and n2 are the total number of N-grams with exactly one and two counts, respectively. 
Chen and Goodman's modified Kneser-Ney discounting (-kndiscount) uses three discounting constants for each N-gram order, one for one-count N-grams, one for two-count N-grams, and one for three-plus-count N-grams:

	Y   = n1/(n1+2*n2)
	D1  = 1 - 2Y(n2/n1)
	D2  = 2 - 3Y(n3/n2)
	D3+ = 3 - 4Y(n4/n3)

4.When I'm testing -kndiscount? Why the counts in the debug info not equal with the actual data?
Warning:
SRILM implements Kneser-Ney discounting by actually modifying the counts of the lower order N-grams. Thus, when the -write option is used to write the counts with -kndiscount or -ukndiscount, only the highest order N-grams and N-grams that start with <s> will have their regular counts c(a_z), all others will have the modified counts n(*_z) instead. See Warning 2 in the next section.
Warning 2
When -kndiscount or -ukndiscount options are used, the count file contains modified counts. Specifically, all N-grams of the maximum order, and all N-grams that start with <s> have their regular counts c(a_z), but shorter N-grams that do not start with <s> have the number of unique words preceding them n(*a_z) instead. See the description of -kndiscount and -ukndiscount for details.

5.Can I use the unigram model in the order-2 kndiscount output?
MAILING TO MAILLIST
Hello
When I use order 2 kndiscount, I get a unigram model and a bigram model
Then I use order 1 kndiscount, I also get a unigram
But these two unigrams are different, I read the 
http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html
(The same situation happens when I try order-3 and order-2)
It seems that this has to do with some implementation issue, what i want to ask is, is the unigram I get in the order 2 kndiscount uncorrect?
Because if I use order1 Katz discount and order 2 Katz discount, the two unigrams are the same, so I think I need to treat kndiscount result with caution.
Many thanks
Goose
Andreas Stolcke:
It is one of the distinguishing features of KN discounting that the lower-order (backoff) distributions are estimated differently from the highest-order distribution.
You are not supposed to use the unigram distribution in a KN-smoothed bigram by itself. 
So what you're seeing is completely expected and correct.   For a detailed explanation see the Chen and Goodman paper.

6.Is -interpolate -kndiscount and -kndiscount the same?
No, they produce different results.

7.In debug info, why does something like 'discarded 15,004,868 2-gram probs discounted to zero' happen?
That's because -gtmin , try -gtnmin 1 to disable this discounting.

8.Is the -memuse correct? What does the "wasted memory" mean?
> Hi,
> I'm a french PhD Student, using the toolkit to compute ngram and
> class-ngram models on Hub4 and Hub5 data.
> I recently tried to mix several models with ngram -mix-lm, which works
> fine except for big models (learned on Hub4).
> It seems to be matter of memory. So I used the -memuse option to have an
> idea of the memory load.
> But this option doesn't reflect the actual load of the memory. It says
> 900M when a top running of the same machine gives a amount a 2,5G used.
That's because -memuse only calculates the memory used by the final model.
For static interpolation with -mix-lm the program needs to temporarily 
allocate both the input models and the resulting mixture model, so 2.5 GB
doesn't sound too outlandish.
(I know one could implement this operation without requiring all models
to be fully in memory, but i preferred to keep the code simple.)
> So my 2 questions are :
> - is it normal that the -memuse option gives a wrong result ?
see above.
> - is it normal that the toolkit use so much memory, or have I done
> something wrong in the installation ?
The default build optimizes data structures for speed, not space.
that's why you see a significant portion of memory "wasted" (according to
-memuse output).  That's the extra space needed to keep hash tables sparse.
As of SRILM version 1.3.2, you can build a separate version of the binaries
optimized for space, and that's usually worth it once you start dealing with
Hub4 ;-)  Follow the instructions under item 9 in the INSTALL file.
--Andreas

About the options:
	-lm lmfile
Estimate a language model from the total counts and write it to lmfile. This option applies to several LM model types (see below) but the default is to estimate a backoff N-gram model, and write it in ngram-format(5).
	the default discounting method
default If the user does not specify any discounting options, ngram-count uses Good-Turing discounting (aka Katz smoothing) by default. The Good-Turing estimate states that for any N-gram that occurs r times, we should pretend that it occurs r' times where  r' = (r+1) n[r+1]/n[r]

===ngram===
important options:
	-prune threshold
Prune N-gram probabilities if their removal causes (training set) perplexity of the model to increase by less than threshold relative.

=EXAMPLE PURE MLE MODEL=
related:
http://www.speech.sri.com/pipermail/srilm-user/2004q2/000218.html
test_data:
<s> Alice loves Bob </s>
<s> Alice loves Bob very much </s>
<s> Bob loves Mary </s>
use:
	ngram-count -write allcnt -order 3 -debug 2 -text test_htx.dat -gt1max 0 -gt1min 1 -gt2max 0 -gt2min 1 -gt3max 0 -gt3min 1 -lm lmtest
The gtmin and gtmax are wrote to make sure the GT doesn't do discount.
There is another way to explicitly use the addsmooth model.
	ngram-count -write allcnt -order 3 -debug 1 -addsmooth 0 -text test_htx.dat -gt1min 1 -gt2min 1 -gt3min 1 -lm lmtest
Beware the default gt3min is 2, so we need to set it even if we use the addsmooth smoothing method.
I get:

\data\
ngram 1=6
ngram 2=8
ngram 3=6

\1-grams:
-0.60206	</s>
-99	<s>	-7.277691
-0.90309	Alice	-99
-0.60206	Bob	-7.180781
-0.90309	Mary	-99
-0.60206	loves	-7.277691

\2-grams:
-0.30103	<s> Alice	0
-0.30103	<s> Bob	-99
0	Alice loves	-99
-0.30103	Bob </s>
-0.30103	Bob loves	-99
0	Mary </s>
-0.30103	loves Bob	-99
-0.30103	loves Mary	0

\3-grams:
0	<s> Alice loves
0	<s> Bob loves
0	loves Bob </s>
0	loves Mary </s>
0	Alice loves Bob
0	Bob loves Mary

\end\

===EXPERIMENT1:redo Kai's script===
1.
---DATA---
wocab : 50k words 
text : 310,660,648 lines 
model : 1-gram

------train-1-kn------
time ngram-count -vocab weibo.50k.wlist -order 1 -gt1min 1 -kndiscount1 -debug 1 -interpolate -text textsources/weibo.dat -lm weiboLM-1-GRAM -memuse

result:
total memory : 3113168 (2.96895M)
total time : 5m52.63s
debug 1

------train-2-kn--------
time ngram-count -vocab weibo.50k.wlist -order 2 -gt1min 1 -gt2min 2 -kndiscount1 -kndiscount2 -debug 1 -interpolate -text textsources/weibo.dat -lm weibo.bigram -memuse

result :
computer:lagrange
total memory 1383665856 (1319.57M), used 767072752 (731.538M), wasted 616593104 (588.029M)
total time : 8m56.66s
weibo.bigram with 16,827,647 lines
debug 2

-------train-3-kn--------
(time /slfs6/users/txh18/software/SRILM/bin/i686-m64/ngram-count -vocab textsources/weibo.50k.wlist -order 3 -gt1min 1 -gt2min 2 -gt3min 2 -kndiscount1 -kndiscount2 -kndiscount3 -debug 1 -interpolate -text textsources/weibo.dat -lm weibo-3-GRAM -memuse 1>> weibo-3-gram.log 2>> weibo-3-gram.log) 2>> weibo-3-GRAM.time

result1 :
computer:lagrange
unknown, due to memory full (guess : using swap) and electricity fault
But I have Kai's result:
weibo.trigram with 75,153,206 lines
result2 :
computer:neuman
total memory 8603763968 (8205.19M), used 5327410176 (5080.61M), wasted 3276353792 (3124.57M)
total time : 26m13.477s
weibo-3-GRAM with 75,153,206 lines, exactly the same with Kai's result
debug 3

-----prune-2-kn----------
time ngram -memuse -debug 1 -lm weibo.bigram -write-lm weibo.bigram.1.0e-7 -order 3 -prune 1.0e-7
result :
total memory 246081452 (234.682M), used 20196124 (19.2605M), wasted 225885328 (215.421M)
total time : 15.358s
weibo.bigram.1.0e-7 with 2304534 lines

-----prune-3-kn----------
time ngram -memuse -debug 1 -lm weibo.trigram -write-lm weibo.trigram.1.0e-7 -order 3 -prune 1.0e-7

result:
computer:lagrange
total memory 1390182208 (1325.78M), used 85936408 (81.9553M), wasted 1304245800 (1243.83M)
total time : 1m33.454s
weibo.trigram.1.0e-7 4702423 lines
debug 4


=========================================kndiscount-interpolate-EXPLORE=====================
-the manual says:
kndiscount and -ukndiscount
Kneser-Ney discounting. This is similar to absolute discounting in that the discounted probability is computed by subtracting a constant D from the N-gram count. The options -kndiscount and -ukndiscount differ as to how this constant is computed. 
The main idea of Kneser-Ney is to use a modified probability estimate for lower order N-grams used for backoff. Specifically, the modified probability for a lower order N-gram is taken to be proportional to the number of unique words that precede it in the training data. With discounting and normalization we get:
	f(a_z) = (c(a_z) - D0) / c(a_) 	;; for highest order N-grams
	f(_z)  = (n(*_z) - D1) / n(*_*)	;; for lower order N-grams
where the n(*_z) notation represents the number of unique N-grams that match a given pattern with (*) used as a wildcard for a single word. D0 and D1 represent two different discounting constants, as each N-gram order uses a different discounting constant. The resulting conditional probability and the backoff weight is calculated as given in equations (2) and (3):
	p(a_z)  = (c(a_z) > 0) ? f(a_z) : bow(a_) p(_z)     ; Eqn.2
	bow(a_) = (1 - Sum_Z1 f(a_z)) / (1 - Sum_Z1 f(_z))  ; Eqn.3
The option -interpolate is used to create the interpolated versions of -kndiscount and -ukndiscount. In this case we have:
	p(a_z) = g(a_z) + bow(a_) p(_z)  ; Eqn.4
Let Z1 be the set {z: c(a_z) > 0}. For highest order N-grams we have:
	g(a_z)  = max(0, c(a_z) - D) / c(a_)
	bow(a_) = 1 - Sum_Z1 g(a_z)
	        = 1 - Sum_Z1 c(a_z) / c(a_) + Sum_Z1 D / c(a_)
	        = D n(a_*) / c(a_)
Let Z2 be the set {z: n(*_z) > 0}. For lower order N-grams we have:
	g(_z)  = max(0, n(*_z) - D) / n(*_*)
	bow(_) = 1 - Sum_Z2 g(_z)
	       = 1 - Sum_Z2 n(*_z) / n(*_*) + Sum_Z2 D / n(*_*)
	       = D n(_*) / n(*_*)
The original Kneser-Ney discounting (-ukndiscount) uses one discounting constant for each N-gram order. These constants are estimated as
	D = n1 / (n1 + 2*n2)
where n1 and n2 are the total number of N-grams with exactly one and two counts, respectively. 
Chen and Goodman's modified Kneser-Ney discounting (-kndiscount) uses three discounting constants for each N-gram order, one for one-count N-grams, one for two-count N-grams, and one for three-plus-count N-grams:
	Y   = n1/(n1+2*n2)
	D1  = 1 - 2Y(n2/n1)
	D2  = 2 - 3Y(n3/n2)
	D3+ = 3 - 4Y(n4/n3)






ngram-count -order 3 -gt1min 1 -gt2min 1 -gt3min 1 -text test_htx.dat -write1 cnt1 -write2 cnt2 -write3 cnt3 -kndiscount1 -kndiscount2 -kndiscount3 -debug 1 -lm lmtest

text_htx.dat:
<s> Alice loves Bob </s>
<s> Alice also loves Kai </s>
<s> Kai loves KaiKai </s>
<s> Kai also loves Alice </s>
<s> KK hates Alice </s>
<s> KK also hates Bob </s>
<s> KK KK YY </s>
<s> loves YY </s>
<s> Miss YY </s>
<s> KaiKai YY </s>
<s> YY KK </s>
<s> MM </s>
<s> b3 a3 a3 a3 </s>
<s> c3 a3 a3 a3 </s>
<s> b3 b3 b3 </s>
<s> a3 b3 b3 b3 </s>
<s> d3 b3 b3 b3 </s>
<s> a3 d3 b3 </s>
<s> a3 Miss YY </s>
<s> b3 Miss YY </s>
<s> c3 Miss YY </s>
<s> d3 KaiKai YY </s>

warning: distributing 0.538462 left-over probability mass over all 16 words
n1 = 2
n2 = 4
n3 = 4
n4 = 4
D1 = 0.2
D2 = 1.4
D3+ = 2.2

\data\
ngram 1=17
ngram 2=52
ngram 3=57

\1-grams:
-0.7840372	</s>
-99	<s>	0.2318284
-1.309463	Alice	-0.05883086
-1.344936	Bob	-0.1308702
-1.309463	KK	0.02208804
-1.344936	Kai	-0.05258189
-1.309463	KaiKai	-0.07859299
-1.309463	MM	-0.1182811
-1.165775	Miss	-0.5245043
-1.057992	YY	-0.3573291
-1.165775	a3	-0.05201465
-1.309463	also	-0.1523443
-1.165775	b3	-0.146658
-1.309463	c3	-0.1325376
-1.344936	d3	-0.1504561
-1.344936	hates	-0.1533122
-1.165775	loves	-0.05605629

-Let's do some trying:
for d3 in 1-gram:
g(_z) = max(0, n(*_z) - D) / n(*_*)
g(_z) = (2 - 1.4) / 74 + 0.538462 / 16 = 0.04176198310810811
and 10 ** -1.344936 = 0.04519225372342939, this seems ok
for Alice in 1-gram:
g(_z) = max(0, n(*_z) - D) / n(*_*)
g(_z) = (3 - 2.2) / 74 + 0.538462 / 16 = 0.04446468581081081
and 10 ** -1.309463 = 0.04903847996632592, this seems ok


\2-grams:
-1.459536	<s> Alice	0.0388455
-1.061596	<s> KK	0.05858956
-1.459536	<s> Kai	0.07226925
-1.781755	<s> KaiKai	0.07927416
-1.781755	<s> MM	0.14799
-1.781755	<s> Miss	0.5159668
-1.781755	<s> YY	-0.01551216
-1.061596	<s> a3	0.02527268
-1.061596	<s> b3	0.2149368
-1.459536	<s> c3	0.14799
-1.459536	<s> d3	0.156342
-1.781755	<s> loves	-0.01551216
-0.7191734	Alice </s>
-1.041393	Alice also	0.07927416
-1.041393	Alice loves	-0.01551216
-0.4181434	Bob </s>
-1.138303	KK </s>
-1.138303	KK KK	-0.01551216
-1.138303	KK YY	0.3303302
-1.138303	KK also	0.007811259
-1.138303	KK hates	0.03884549
-0.916454	Kai </s>
-0.916454	Kai also	0.07927416
-0.916454	Kai loves	-0.01551216
-0.916454	KaiKai </s>
-0.5942346	KaiKai YY	-0.5990888
-0.4393327	MM </s>
-0.1383027	Miss YY	-0.1404509
-0.2352127	YY </s>
-1.138303	YY KK	-0.01551216
-1.284431	a3 </s>
-1.284431	a3 Miss	0.5159668
-0.5642715	a3 a3	-0.8072361
-1.284431	a3 b3	0.14799
-1.284431	a3 d3	0.07927416
-0.916454	also hates	0.03884549
-0.5942346	also loves	0.01996016
-1.020203	b3 </s>
-1.342423	b3 Miss	0.5159668
-1.342423	b3 a3	0.08999801
-0.4393327	b3 b3	-0.1272714
-0.7403627	c3 Miss	0.5159668
-0.7403627	c3 a3	0.08999801
-0.916454	d3 KaiKai	0.07927416
-0.5942346	d3 b3	0.218571
-0.7403627	hates Alice	0.043698
-0.7403627	hates Bob	0.1605791
-1.138303	loves Alice	0.043698
-1.138303	loves Bob	0.1605791
-1.138303	loves Kai	0.007811259
-1.138303	loves KaiKai	0.007811259
-1.138303	loves YY	0.3303302

\3-grams:
-1.278754	<s> Alice also
-1.278754	<s> Alice loves
-0.9777236	hates Alice </s>
-0.9777236	loves Alice </s>
-0.9777236	hates Bob </s>
-0.9777236	loves Bob </s>
-1.454845	<s> KK KK
-1.454845	<s> KK also
-1.454845	<s> KK hates
-0.9777236	KK KK YY
-0.9777236	YY KK </s>
-1.278754	<s> Kai also
-1.278754	<s> Kai loves
-0.9777236	loves Kai </s>
-0.9777236	<s> KaiKai YY
-0.9777236	d3 KaiKai YY
-0.9777236	loves KaiKai </s>
-0.9777236	<s> MM </s>
-0.9777236	<s> Miss YY
-0.9777236	a3 Miss YY
-0.9777236	b3 Miss YY
-0.9777236	c3 Miss YY
-0.9777236	<s> YY KK
-0.9777236	KK YY </s>
-0.04830468	KaiKai YY </s>
-0.1565377	Miss YY </s>
-0.9777236	loves YY </s>
-1.454845	<s> a3 Miss
-1.454845	<s> a3 b3
-1.454845	<s> a3 d3
-0.3493347	a3 a3 </s>
-0.3493347	a3 a3 a3
-0.9777236	b3 a3 a3
-0.9777236	c3 a3 a3
-0.9777236	Alice also loves
-0.9777236	KK also hates
-0.9777236	Kai also loves
-1.454845	<s> b3 Miss
-1.454845	<s> b3 a3
-1.454845	<s> b3 b3
-0.9777236	a3 b3 b3
-0.5254259	b3 b3 </s>
-0.5254259	b3 b3 b3
-1.278754	d3 b3 </s>
-1.278754	d3 b3 b3
-1.278754	<s> c3 Miss
-1.278754	<s> c3 a3
-1.278754	<s> d3 KaiKai
-1.278754	<s> d3 b3
-0.9777236	a3 d3 b3
-0.9777236	KK hates Alice
-0.9777236	also hates Bob
-0.9777236	<s> loves YY
-0.9777236	Alice loves Bob
-0.9777236	Kai loves KaiKai
-1.278754	also loves Alice
-1.278754	also loves Kai

\end\


==========================================debugs=============================================

debug 1 : 
using ModKneserNey for 1-grams
Kneser-Ney smoothing 1-grams
warning: 2.09568e-07 backoff probability mass left for "" -- disabling interpolation
discarded 2 1-gram probs predicting pseudo-events
warning: distributing 4.33012e-05 left-over probability mass over 242 zeroton words

debug 2 : 
discarded 2 2-gram contexts containing pseudo-events
discarded 32987 2-gram probs predicting pseudo-events
discarded 15,004,868 2-gram probs discounted to zero
writing 50002 1-grams
writing 16,777,635 2-grams

debug 3 :
textsources/weibo.dat: line 310660648: 3.10661e+08 sentences, 1.64702e+09 words, 1.22293e+07 OOVs
0 zeroprobs, logprob= 0 ppl= 1 ppl1= 1
total memory 8603763968 (8205.19M), used 5327410176 (5080.61M), wasted 3276353792 (3124.57M)
allocations of size 32: 18889718
allocations of size 56: 4752422
allocations of size 104: 3413169
allocations of size 200: 1814778
allocations of size >= 256: 2976128
1 blocks of 2-word chunks
1 blocks of 3-word chunks
577 blocks of 4-word chunks
1 blocks of 5-word chunks
254 blocks of 7-word chunks
1 blocks of 9-word chunks
339 blocks of 13-word chunks
using ModKneserNey for 1-grams
modifying 1-gram counts for Kneser-Ney smoothing
Kneser-Ney smoothing 1-grams
n1 = 280
n2 = 309
n3 = 295
n4 = 289
D1 = 0.311804
D2 = 1.10697
D3+ = 1.77815
using ModKneserNey for 2-grams
modifying 2-gram counts for Kneser-Ney smoothing
Kneser-Ney smoothing 2-grams
n1 = 18728154
n2 = 4715747
n3 = 2170478
n4 = 1259321
D1 = 0.665071
D2 = 1.08168
D3+ = 1.45649
using ModKneserNey for 3-grams
Kneser-Ney smoothing 3-grams
n1 = 118544951
n2 = 23004014
n3 = 9587553
n4 = 5407677
D1 = 0.720406
D2 = 1.09925
D3+ = 1.37467
discarded 1 1-gram probs predicting pseudo-events
warning: distributing 1.33717e-05 left-over probability mass over 242 zeroton words
warning: 1.85186e-06 backoff probability mass left for "<s>" -- disabling interpolation
discarded 2 2-gram contexts containing pseudo-events
discarded 32987 2-gram probs predicting pseudo-events
discarded 18723781 2-gram probs discounted to zero
warning: 2.91694e-06 backoff probability mass left for "私 优化" -- disabling interpolation
warning: 2.84076e-06 backoff probability mass left for "添 理想" -- disabling interpolation
warning: 2.87504e-06 backoff probability mass left for "添 理想" -- incrementing denominator
warning: 2.86621e-06 backoff probability mass left for "真诚 未来" -- disabling interpolation
warning: 2.8754e-06 backoff probability mass left for "真诚 未来" -- incrementing denominator
discarded 112230 3-gram contexts containing pseudo-events
discarded 902801 3-gram probs predicting pseudo-events
discarded 117375559 3-gram probs discounted to zero
inserted 3035277 redundant 2-gram probs
writing 50002 1-grams
writing 16093999 2-grams
writing 59009192 3-grams

debug 4 : 
reading 50002 1-grams
reading 16093999 2-grams
reading 59009192 3-grams
pruned 56846800 3-grams
pruned 13603983 2-grams
total memory 1390182208 (1325.78M), used 85936408 (81.9553M), wasted 1304245800 (1243.83M)
allocations of size 12: 178993
allocations of size 20: 167715
allocations of size 36: 235337
allocations of size 40: 689
allocations of size 68: 193559
allocations of size 72: 862
allocations of size 132: 161099
allocations of size 136: 1816
allocations of size >= 256: 439051
118 blocks of 2-word chunks
41 blocks of 3-word chunks
47 blocks of 5-word chunks
43 blocks of 9-word chunks
writing 50002 1-grams
writing 2490016 2-grams
writing 2162392 3-grams

